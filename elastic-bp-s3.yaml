apiVersion: cr.kanister.io/v1alpha1
kind: Blueprint
metadata: 
  name: elasticsearch-incremental-blueprint 
actions:
  backup:
    outputArtifacts:
      s3Snap:
        keyValue:
          repoPath: '{{ .Phases.setupNodeKeyStoreBackup.Output.repoPath }}'
          snapshotName: '{{ .Phases.setupNodeKeyStoreBackup.Output.snapshotName }}'
          esClusterName: '{{ .Phases.setupNodeKeyStoreBackup.Output.esClusterName }}'
          esClusterNamespace: '{{ .Object.metadata.namespace }}'
    phases:  
    - func: Wait
      name: waitElasticGreenBackup
      args:
        timeout: 360s
        conditions:
          anyOf:
            - condition: '{{ if  or (eq "{ $.status.health }" "green") (eq "{ $.status.health }" "yellow")}}true{{ else }}false{{ end }}'            
              objectReference:
                apiVersion: v1
                group: elasticsearch.k8s.elastic.co
                resource: elasticsearches
                name: "{{ .Object.metadata.name }}"
                namespace: "{{ .Object.metadata.namespace}}"  
    - func: KubeTask
      name: setupNodeKeyStoreBackup
      objects:
        elasticSecret:
          kind: Secret
          name: '{{ .Object.metadata.name }}-es-elastic-user'
          namespace: '{{ .Object.metadata.namespace }}' 
      args:        
        image: ghcr.io/kanisterio/kanister-kubectl-1.18:0.109.0
        command:
        - bash
        - -x
        - -o
        - errexit
        - -o
        - pipefail
        - -c
        - |                     
          PROFILE_TYPE="{{ .Profile.Location.Type }}"
          if [[ $PROFILE_TYPE != "s3Compliant" ]]
          then 
            echo "Only s3Compliant profile are supported, exiting"
            exit 3
          fi
          CLUSTER_UID=$(kubectl get ns default -o jsonpath='{.metadata.uid}')
          NS_UID=$(kubectl get ns {{ .Object.metadata.namespace }} -o jsonpath='{.metadata.uid}')
          ES_CLUSTER_UID={{ .Object.metadata.uid }}
          REPO_PATH=k10/$CLUSTER_UID/elastic-search/$NS_UID/$ES_CLUSTER_UID
          SNAPSHOT_NAME="snap_{{ toDate "2006-01-02T15:04:05.999999999Z07:00" .Time | date "2006-01-02t15:04:05z07:00" }}"
          echo "output the snapshot info $SNAPSHOT_NAME" 
          kando output repoPath $REPO_PATH 
          kando output snapshotName $SNAPSHOT_NAME 
          kando output esClusterName {{ .Object.metadata.name }}

          # setup the credentials on the keystore setting of each nodes 
          {{- if .Profile.Credential.KeyPair }}
          AWS_SECRET_KEY="{{ .Profile.Credential.KeyPair.Secret }}"
          AWS_ACCESS_KEY="{{ .Profile.Credential.KeyPair.ID }}"
          {{- else }}
          AWS_SECRET_KEY="{{ .Profile.Credential.Secret.Data.aws_secret_access_key | toString }}"
          AWS_ACCESS_KEY="{{ .Profile.Credential.Secret.Data.aws_access_key_id | toString }}"
          {{- end }}
          
          NS_NAME={{ .Object.metadata.namespace }}
          ES_CLUSTER_NAME={{ .Object.metadata.name }}
          {{- range $index, $nodeSet := .Object.spec.nodeSets }}
            NODE_SET_COUNT={{ $nodeSet.count }}
            NODE_SET_COUNT=$((NODE_SET_COUNT-1))
            NODE_SET_NAME={{ $nodeSet.name }}
            for i in $(seq 0 $NODE_SET_COUNT)
            do 
                pod="$ES_CLUSTER_NAME-es-$NODE_SET_NAME-$i"
                echo "updating elasticsearch-keystore of $pod"
                kubectl exec -it -n $NS_NAME $pod -c elasticsearch -- bash -c "echo ${AWS_ACCESS_KEY} | /usr/share/elasticsearch/bin/elasticsearch-keystore add --stdin -f s3.client.default.access_key" 
                kubectl exec -it -n $NS_NAME $pod -c elasticsearch -- bash -c "echo ${AWS_SECRET_KEY} | /usr/share/elasticsearch/bin/elasticsearch-keystore add --stdin -f s3.client.default.secret_key"
            done 
          {{- end }}
    - func: KubeTask
      name: snapshotElastic     
      args:
        namespace: "{{ .Object.metadata.namespace }}"                
        image: ghcr.io/kanisterio/kanister-kubectl-1.18:0.109.0
        command:
        - bash
        - -x
        - -o
        - errexit
        - -o
        - pipefail
        - -c
        - |                               
          ES_URL="https://{{ .Object.metadata.name }}-es-http:9200"
          PASSWORD="{{ index .Phases.setupNodeKeyStoreBackup.Secrets.elasticSecret.Data "elastic" | toString }}"
          REGION="{{ .Profile.Location.Region }}"
          BUCKET="{{ .Profile.Location.Bucket }}"
          ENDPOINT="{{ .Profile.Location.Endpoint }}"
          if [[ -z $ENDPOINT ]] 
          then 
             ENDPOINT="s3.amazonaws.com"
             path_style_access="false"
          else
              # when using non amazon s3 endpoint, we need to use path style access
              path_style_access="true"
          fi
          REPO_PATH="{{ .Phases.setupNodeKeyStoreBackup.Output.repoPath }}"
          SNAPSHOT_NAME="{{ .Phases.setupNodeKeyStoreBackup.Output.snapshotName }}"
          # reload the secure settings to access the S3 profile
          curl -k -u "elastic:$PASSWORD" -X POST "${ES_URL}/_nodes/reload_secure_settings?pretty" -H 'Content-Type: application/json' -d'
          {}
          '
          echo "Creating the repo"
          curl -k -u "elastic:$PASSWORD" -X PUT "${ES_URL}/_snapshot/k10_repo?pretty" -H 'Content-Type: application/json' -d'
          {
            "type": "s3",
            "settings": {    
              "bucket": "'$BUCKET'",
              "endpoint": "'$ENDPOINT'",
              "region": "'$REGION'",
              "base_path": "'$REPO_PATH'",
              "path_style_access": "'$path_style_access'"          
            }
          }
          '
          echo "creating the snap $SNAPSHOT_NAME" 
          code=$(curl -o /dev/null -w "%{http_code}\n" -k -u "elastic:$PASSWORD" -X PUT "${ES_URL}/_snapshot/k10_repo/$SNAPSHOT_NAME?pretty")
          if [[ $code -eq 200 ]]
          then 
            echo "snapshot $SNAPSHOT_NAME was successful initiated"
          else
            echo "snapshot $SNAPSHOT_NAME was not successful initiated"
            exit 1
          fi

          while curl -k -u "elastic:$PASSWORD" -X GET "${ES_URL}/_snapshot/k10_repo/$SNAPSHOT_NAME/_status?pretty" | grep -P "(IN_PROGRESS|STARTED)"
          do 
            echo "snapshot $SNAPSHOT_NAME still in progress"
            size_in_bytes=$(curl -k -u "elastic:$PASSWORD" -X GET "${ES_URL}/_snapshot/k10_repo/$SNAPSHOT_NAME/_status?pretty" |grep size_in_bytes)
            echo $size_in_bytes
            sleep 4
          done 

          if  curl -k -u "elastic:$PASSWORD" -X GET "${ES_URL}/_snapshot/k10_repo/$SNAPSHOT_NAME/_status?pretty" | grep SUCCESS
          then 
             echo "snapshot $SNAPSHOT_NAME was successful"
          else
             echo "snapshot $SNAPSHOT_NAME was not successful"
             reason=$(curl -k -u "elastic:$PASSWORD" -X GET "${ES_URL}/_snapshot/k10_repo/$SNAPSHOT_NAME/_status?pretty")
             echo $reason
             exit 1
          fi
          sleep 5
  restore:
    inputArtifactNames: 
    - s3Snap  # use it with  for instance {{ .ArtifactsIn.s3Snap.KeyValue.repoPath }} or {{ .ArtifactsIn.s3Snap.KeyValue.snapshotName }}
    phases:   
    - func: Wait
      name: waitElasticGreenRestore
      args:
        timeout: 360s
        conditions:
          anyOf:
            - condition: '{{ if  or (eq "{ $.status.health }" "green") (eq "{ $.status.health }" "yellow")}}true{{ else }}false{{ end }}'            
              objectReference:
                apiVersion: v1
                group: elasticsearch.k8s.elastic.co
                resource: elasticsearches
                name: "{{ .Object.metadata.name }}"
                namespace: "{{ .Object.metadata.namespace}}" 
    - func: ScaleWorkload
      name: ensureSTSIsScaled
      args:
        namespace: "{{ .Object.metadata.namespace}}"  
        kind: statefulset
        name: "{{ .Object.metadata.name }}-es-{{ (index .Object.spec.nodeSets 0).name }}"
        replicas: "{{ (index .Object.spec.nodeSets 0).count }}"  
    - func: KubeTask
      name: setupNodeKeyStoreRestore
      objects:
        elasticSecret:
          kind: Secret
          name: '{{ .Object.metadata.name }}-es-elastic-user'
          namespace: '{{ .Object.metadata.namespace }}' 
      args:        
        image: ghcr.io/kanisterio/kanister-kubectl-1.18:0.109.0
        command:
        - bash
        - +x
        - -o
        - errexit
        - -o
        - pipefail
        - -c
        - |                      
          PROFILE_TYPE="{{ .Profile.Location.Type }}"
          if [[ $PROFILE_TYPE != "s3Compliant" ]]
          then 
            echo "Only s3Compliant profile are supported, exiting"
            exit 3
          fi          
          # setup the credentials on the keystore setting of each nodes 
          {{- if .Profile.Credential.KeyPair }}
          AWS_SECRET_KEY="{{ .Profile.Credential.KeyPair.Secret }}"
          AWS_ACCESS_KEY="{{ .Profile.Credential.KeyPair.ID }}"
          {{- else }}
          AWS_SECRET_KEY="{{ .Profile.Credential.Secret.Data.aws_secret_access_key | toString }}"
          AWS_ACCESS_KEY="{{ .Profile.Credential.Secret.Data.aws_access_key_id | toString }}"
          {{- end }}

          NS_NAME={{ .Object.metadata.namespace }}
          ES_CLUSTER_NAME={{ .Object.metadata.name }}
          {{- range $index, $nodeSet := .Object.spec.nodeSets }}
            NODE_SET_COUNT={{ $nodeSet.count }}
            NODE_SET_COUNT=$((NODE_SET_COUNT-1))
            NODE_SET_NAME={{ $nodeSet.name }}
            for i in $(seq 0 $NODE_SET_COUNT)
            do 
                pod="$ES_CLUSTER_NAME-es-$NODE_SET_NAME-$i"
                echo "updating elasticsearch-keystore of $pod"
                kubectl exec -it -n $NS_NAME $pod -c elasticsearch -- bash -c "echo ${AWS_ACCESS_KEY} | /usr/share/elasticsearch/bin/elasticsearch-keystore add --stdin -f s3.client.default.access_key" 
                kubectl exec -it -n $NS_NAME $pod -c elasticsearch -- bash -c "echo ${AWS_SECRET_KEY} | /usr/share/elasticsearch/bin/elasticsearch-keystore add --stdin -f s3.client.default.secret_key"
            done 
          {{- end }}                         
    - func: KubeTask
      name: restoreElastic     
      args:
        namespace: "{{ .Object.metadata.namespace }}"                
        image: ghcr.io/kanisterio/kanister-kubectl-1.18:0.109.0
        command:
        - bash
        - +x
        - -o
        - errexit
        - -o
        - pipefail
        - -c
        - |                                
          ES_URL="https://{{ .Object.metadata.name }}-es-http:9200"
          PASSWORD="{{ index .Phases.setupNodeKeyStoreRestore.Secrets.elasticSecret.Data "elastic" | toString }}"
          REGION="{{ .Profile.Location.Region }}"
          BUCKET="{{ .Profile.Location.Bucket }}"
          ENDPOINT="{{ .Profile.Location.Endpoint }}"
          if [[ -z $ENDPOINT ]] 
          then 
             ENDPOINT="s3.amazonaws.com"
             path_style_access="false"
          else
              # when using non amazon s3 endpoint, we need to use path style access
              path_style_access="true"
          fi
          REPO_PATH="{{ .ArtifactsIn.s3Snap.KeyValue.repoPath }}"
          SNAPSHOT_NAME="{{ .ArtifactsIn.s3Snap.KeyValue.snapshotName }}"
          # reload the secure settings to access the S3 profile
          curl -k -u "elastic:$PASSWORD" -X POST "${ES_URL}/_nodes/reload_secure_settings?pretty" -H 'Content-Type: application/json' -d'
          {}
          '
          
          # create the repo
          curl -k -u "elastic:$PASSWORD" -X PUT "${ES_URL}/_snapshot/k10_repo?pretty" -H 'Content-Type: application/json' -d'
          {
            "type": "s3",
            "settings": {    
              "bucket": "'$BUCKET'",
              "endpoint": "'$ENDPOINT'",
              "region": "'$REGION'",
              "base_path": "'$REPO_PATH'",
              "path_style_access": "'$path_style_access'"          
            }
          }
          '
         
          echo "stop service geoip, machine learning, monitoring and watcher"
          curl  -k -u "elastic:$PASSWORD"  -X PUT "${ES_URL}/_cluster/settings?pretty" -H 'Content-Type: application/json' -d'
          {
            "persistent": {
              "ingest.geoip.downloader.enabled": false
            }
          }
          '
          curl  -k -u "elastic:$PASSWORD"  -X POST "${ES_URL}/_ilm/stop?pretty"
          curl  -k -u "elastic:$PASSWORD"  -X POST "${ES_URL}/_ml/set_upgrade_mode?enabled=true&pretty"
          curl  -k -u "elastic:$PASSWORD"  -X PUT "${ES_URL}/_cluster/settings?pretty" -H 'Content-Type: application/json' -d'
          {
            "persistent": {
              "xpack.monitoring.collection.enabled": false
            }
          }
          '
          curl  -k -u "elastic:$PASSWORD"  -X POST "${ES_URL}/_watcher/_stop?pretty"
          
          echo "delete all datastreams and indices" 
          curl  -k -u "elastic:$PASSWORD" -X PUT "${ES_URL}/_cluster/settings?pretty" -H 'Content-Type: application/json' -d'
          {
            "persistent": {
              "action.destructive_requires_name": false
            }
          }
          '
          curl  -k -u "elastic:$PASSWORD" -X DELETE "${ES_URL}/_data_stream/*?expand_wildcards=all&pretty"
          curl  -k -u "elastic:$PASSWORD" -X DELETE "${ES_URL}/*?expand_wildcards=all&pretty"

          echo "restore all"
          # TODO manage the monitoring of the restoration process          
          curl  -k -u "elastic:$PASSWORD" -X POST "${ES_URL}/_snapshot/k10_repo/$SNAPSHOT_NAME/_restore?pretty" -H 'Content-Type: application/json' -d'
          {
            "indices": "*",
            "include_global_state": true
          }
          '

          echo "Restart service geoip, machine learning, monitoring and watcher"
          curl  -k -u "elastic:$PASSWORD"  -X PUT "${ES_URL}/_cluster/settings?pretty" -H 'Content-Type: application/json' -d'
          {
            "persistent": {
              "ingest.geoip.downloader.enabled": true
            }
          }
          '
          curl  -k -u "elastic:$PASSWORD"  -X POST "${ES_URL}/_ilm/start?pretty"
          curl  -k -u "elastic:$PASSWORD"  -X POST "${ES_URL}/_ml/set_upgrade_mode?enabled=false&pretty"
          curl  -k -u "elastic:$PASSWORD"  -X PUT "${ES_URL}/_cluster/settings?pretty" -H 'Content-Type: application/json' -d'
          {
            "persistent": {
              "xpack.monitoring.collection.enabled": true
            }
          }
          '
          curl  -k -u "elastic:$PASSWORD"  -X POST "${ES_URL}/_watcher/_start?pretty"

          echo "Reenable destructive_requires_name"
          curl  -k -u "elastic:$PASSWORD" -X PUT "${ES_URL}/_cluster/settings?pretty" -H 'Content-Type: application/json' -d'
          {
            "persistent": {
              "action.destructive_requires_name": null
            }
          }
          '
  # limitation : deletion need a cluster with the same name in the same namespace be up and running
  # if you don't have that then just create it and proceed the deletions of the restorepoint
  delete:
    inputArtifactNames: 
    - s3Snap  
    phases:         
    - func: KubeTask
      name: setupNodeKeyStoreDelete
      objects:
        elasticSecret:
          kind: Secret
          name: '{{ .ArtifactsIn.s3Snap.KeyValue.esClusterName }}-es-elastic-user'
          namespace: '{{ .ArtifactsIn.s3Snap.KeyValue.esClusterNamespace }}' 
      args:        
        image: ghcr.io/kanisterio/kanister-kubectl-1.18:0.109.0
        command:
        - bash
        - +x
        - -o
        - errexit
        - -o
        - pipefail
        - -c
        - |                    
          PROFILE_TYPE="{{ .Profile.Location.Type }}"
          if [[ $PROFILE_TYPE != "s3Compliant" ]]
          then 
            echo "Only s3Compliant profile are supported, exiting"
            exit 3
          fi          
          # setup the credentials on the keystore setting of each nodes 
          {{- if .Profile.Credential.KeyPair }}
          AWS_SECRET_KEY="{{ .Profile.Credential.KeyPair.Secret }}"
          AWS_ACCESS_KEY="{{ .Profile.Credential.KeyPair.ID }}"
          {{- else }}
          AWS_SECRET_KEY="{{ .Profile.Credential.Secret.Data.aws_secret_access_key | toString }}"
          AWS_ACCESS_KEY="{{ .Profile.Credential.Secret.Data.aws_access_key_id | toString }}"
          {{- end }}
          NAMESPACE="{{ .ArtifactsIn.s3Snap.KeyValue.esClusterNamespace }}"
          ES_CLUSTER_NAME="{{ .ArtifactsIn.s3Snap.KeyValue.esClusterName }}"
          # exit with an error if cluster does not exist 
          if kubectl get elasticsearch -n $NAMESPACE $ES_CLUSTER_NAME 
          then 
            echo "we have an es cluster $ES_CLUSTER_NAME in ns $NAMESPACE"
          else
            echo "error, we don't have an es cluster $ES_CLUSTER_NAME in ns $NAMESPACE"
            exit 5
          fi
          NS_NAME=$NAMESPACE          
          for NODE_SET_NAME in $(kubectl get elasticsearch -n $NS_NAME $ES_CLUSTER_NAME -o jsonpath='{.spec.nodeSets[*].name}')
          do
            NODE_SET_COUNT=$(kubectl get elasticsearch -n $NS_NAME $ES_CLUSTER_NAME -o jsonpath="{.spec.nodeSets[?(@.name == '$NODE_SET_NAME')].count}")
            NODE_SET_COUNT=$((NODE_SET_COUNT-1))
            echo "$NODE_SET_NAME $NODE_SET_COUNT"
            for i in $(seq 0 $NODE_SET_COUNT)
            do 
                pod="$ES_CLUSTER_NAME-es-$NODE_SET_NAME-$i"
                echo "updating elasticsearch-keystore of $pod"
                kubectl exec -it -n $NS_NAME $pod -c elasticsearch -- bash -c "echo ${AWS_ACCESS_KEY} | /usr/share/elasticsearch/bin/elasticsearch-keystore add --stdin -f s3.client.default.access_key" 
                kubectl exec -it -n $NS_NAME $pod -c elasticsearch -- bash -c "echo ${AWS_SECRET_KEY} | /usr/share/elasticsearch/bin/elasticsearch-keystore add --stdin -f s3.client.default.secret_key"
            done 
          done                             
    - func: KubeTask
      name: deleteSnapshot    
      args:
        namespace: '{{ .ArtifactsIn.s3Snap.KeyValue.esClusterNamespace }}'                
        image: ghcr.io/kanisterio/kanister-kubectl-1.18:0.109.0
        command:
        - bash
        - +x
        - -o
        - errexit
        - -o
        - pipefail
        - -c
        - |                                
          ES_URL="https://{{ .ArtifactsIn.s3Snap.KeyValue.esClusterName }}-es-http:9200"
          PASSWORD="{{ index .Phases.setupNodeKeyStoreDelete.Secrets.elasticSecret.Data "elastic" | toString }}"
          REGION="{{ .Profile.Location.Region }}"
          BUCKET="{{ .Profile.Location.Bucket }}"
          ENDPOINT="{{ .Profile.Location.Endpoint }}"
          if [[ -z $ENDPOINT ]] 
          then 
             ENDPOINT="s3.amazonaws.com"
             path_style_access="false"
          else
              # when using non amazon s3 endpoint, we need to use path style access
              path_style_access="true"
          fi
          REPO_PATH="{{ .ArtifactsIn.s3Snap.KeyValue.repoPath }}"
          SNAPSHOT_NAME="{{ .ArtifactsIn.s3Snap.KeyValue.snapshotName }}"          
          # reload the secure settings to access the S3 profile
          curl -k -u "elastic:$PASSWORD" -X POST "${ES_URL}/_nodes/reload_secure_settings?pretty" -H 'Content-Type: application/json' -d'
          {}
          '
          
          echo "create the repo"
          curl -k -u "elastic:$PASSWORD" -X PUT "${ES_URL}/_snapshot/k10_repo?pretty" -H 'Content-Type: application/json' -d'
          {
            "type": "s3",
            "settings": {    
              "bucket": "'$BUCKET'",
              "endpoint": "'$ENDPOINT'",
              "region": "'$REGION'",
              "base_path": "'$REPO_PATH'",
              "path_style_access": "'$path_style_access'" 
            }
          }
          ' 

          echo "delete the snap $SNAPSHOT_NAME"
          curl -k -u "elastic:$PASSWORD" -X DELETE "${ES_URL}/_snapshot/k10_repo/$SNAPSHOT_NAME?pretty"
          echo "Deletion of $SNAPSHOT_NAME was successful"